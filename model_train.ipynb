{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Emotion Classifier Notebook\n",
                "\n",
                "This notebook implements an emotion classifier for images using a transfer learning approach with ResNet50."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Model, load_model\n",
                "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Activation, BatchNormalization, Conv2D, Input\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
                "from tensorflow.keras.applications import ResNet50\n",
                "from tensorflow.keras.applications.resnet import preprocess_input\n",
                "from tensorflow.keras.losses import CategoricalCrossentropy\n",
                "from tensorflow.keras.metrics import Precision, Recall\n",
                "from tensorflow.keras.utils import image_dataset_from_directory\n",
                "from tensorflow.keras.regularizers import l2\n",
                "from tensorflow.keras import Sequential\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.metrics import classification_report, confusion_matrix, recall_score, accuracy_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "Define the configuration parameters for the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Configuration settings\n",
                "class Config:\n",
                "    BALANCE_PROCESSED_DATA_DIR = \"path/to/your/data_directory\"  # Update this with your data path\n",
                "    MODEL_DIR = \"path/to/save/model\"  # Update this with your model save path\n",
                "\n",
                "# Model parameters\n",
                "data_dir = Config.BALANCE_PROCESSED_DATA_DIR\n",
                "input_shape = (224, 224, 3)\n",
                "num_classes = 3\n",
                "batch_size = 32\n",
                "epochs = 50\n",
                "initial_lr_phase1 = 1e-4\n",
                "initial_lr_phase2 = 1e-5\n",
                "lr_decay_alpha = 1e-6"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Custom F1 Score Metric"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class F1Score(tf.keras.metrics.Metric):\n",
                "    def __init__(self, name='f1_score', **kwargs):\n",
                "        super().__init__(name=name, **kwargs)\n",
                "        self.precision = Precision()\n",
                "        self.recall = Recall()\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
                "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
                "\n",
                "    def result(self):\n",
                "        p = self.precision.result()\n",
                "        r = self.recall.result()\n",
                "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data\n",
                "\n",
                "Function to load and prepare the datasets for training, validation, and testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def load_data(data_dir, input_shape, batch_size):\n",
                "    input_size = (input_shape[0], input_shape[1])\n",
                "    \n",
                "    train_dir = os.path.join(data_dir, \"Train\")\n",
                "    val_dir = os.path.join(data_dir, \"Validation\")\n",
                "    test_dir = os.path.join(data_dir, \"Test\")\n",
                "\n",
                "    train_dataset = image_dataset_from_directory(\n",
                "        train_dir,\n",
                "        image_size=input_size,\n",
                "        batch_size=batch_size,\n",
                "        label_mode=\"categorical\",\n",
                "        shuffle=True\n",
                "    ).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "    val_dataset = image_dataset_from_directory(\n",
                "        val_dir,\n",
                "        image_size=input_size,\n",
                "        batch_size=batch_size,\n",
                "        label_mode=\"categorical\",\n",
                "        shuffle=False\n",
                "    ).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "    test_dataset = image_dataset_from_directory(\n",
                "        test_dir,\n",
                "        image_size=input_size,\n",
                "        batch_size=batch_size,\n",
                "        label_mode=\"categorical\",\n",
                "        shuffle=False\n",
                "    ).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "    print(\"Data loaded successfully!\")\n",
                "    return train_dataset, val_dataset, test_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Print Class Distribution\n",
                "\n",
                "Function to analyze and print the class distribution in the datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def print_class_distribution(train_dataset, val_dataset, test_dataset, num_classes):\n",
                "    def count_class_samples(dataset, dataset_name):\n",
                "        class_counts = np.zeros(num_classes, dtype=int)\n",
                "        total_samples = 0\n",
                "\n",
                "        # Convert dataset to a NumPy array (reduces computation overhead)\n",
                "        for _, y in dataset:\n",
                "            y_indices = tf.argmax(y, axis=1).numpy()  # Convert one-hot to class indices\n",
                "            unique, counts = np.unique(y_indices, return_counts=True)\n",
                "            \n",
                "            for cls, count in zip(unique, counts):\n",
                "                class_counts[cls] += count\n",
                "            \n",
                "            total_samples += len(y_indices)\n",
                "\n",
                "        print(f\"\\n{dataset_name} Data Class Distribution:\")\n",
                "        for i, count in enumerate(class_counts):\n",
                "            percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n",
                "            print(f\"Class {i}: {count} samples ({percentage:.2f}%)\")\n",
                "\n",
                "    if train_dataset: count_class_samples(train_dataset, \"Training\")\n",
                "    if val_dataset: count_class_samples(val_dataset, \"Validation\")\n",
                "    if test_dataset: count_class_samples(test_dataset, \"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build Model\n",
                "\n",
                "Function to build the transfer learning model with ResNet50 as the base model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def build_model(input_shape, num_classes, initial_lr_phase1):\n",
                "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
                "    base_model.trainable = False\n",
                "    \n",
                "    inputs = Input(shape=input_shape)\n",
                "    \n",
                "    augmentation_pipeline = Sequential([\n",
                "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
                "        tf.keras.layers.RandomRotation(0.05),\n",
                "        tf.keras.layers.RandomBrightness(0.1),\n",
                "        tf.keras.layers.RandomContrast(0.1),\n",
                "        tf.keras.layers.RandomZoom(0.0, 0.05)\n",
                "    ], name=\"augmentation_pipeline\")\n",
                "    \n",
                "    x = augmentation_pipeline(inputs)\n",
                "    x = preprocess_input(x)\n",
                "    x = base_model(x, training=False) \n",
                "    \n",
                "    # Modified Head with two Dense blocks\n",
                "    x = GlobalAveragePooling2D()(x)\n",
                "\n",
                "    x = Dense(128, kernel_regularizer=l2(0.0005), kernel_initializer='he_normal')(x)\n",
                "    x = BatchNormalization()(x)\n",
                "    x = Activation('relu')(x)\n",
                "    x = Dropout(0.3)(x)\n",
                "\n",
                "    x = Dense(64, kernel_regularizer=l2(0.0005), kernel_initializer='he_normal')(x)\n",
                "    x = BatchNormalization()(x)\n",
                "    x = Activation('relu')(x)\n",
                "    x = Dropout(0.3)(x)\n",
                "    \n",
                "    predictions = Dense(num_classes, activation='softmax')(x)\n",
                "    \n",
                "    model = Model(inputs=inputs, outputs=predictions)\n",
                "    \n",
                "    # Initial compile, will be recompiled in _train_phase\n",
                "    model.compile(\n",
                "        optimizer=Adam(learning_rate=initial_lr_phase1), # Placeholder LR\n",
                "        loss=CategoricalCrossentropy(label_smoothing=0.05),\n",
                "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), F1Score(name='f1')]\n",
                "    )\n",
                "    \n",
                "    return model, base_model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Function: Get Number of Batches Per Epoch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def get_num_batches_per_epoch(train_dataset, batch_size):\n",
                "    \"\"\"Determines the number of batches per epoch for the training dataset.\"\"\"\n",
                "    num_batches_per_epoch_tensor = tf.data.experimental.cardinality(train_dataset)\n",
                "    if num_batches_per_epoch_tensor < 0: \n",
                "        print(\"Warning: Training dataset cardinality is unknown or infinite. Attempting to infer steps_per_epoch.\")\n",
                "        # This fallback is inefficient and should ideally be avoided by ensuring\n",
                "        # the dataset (e.g., from image_dataset_from_directory) has a known cardinality.\n",
                "        try:\n",
                "            num_total_samples = 0\n",
                "            for _ in train_dataset.unbatch().batch(1): \n",
                "                num_total_samples +=1\n",
                "            if num_total_samples == 0:\n",
                "                raise ValueError(\"Could not determine number of samples from the dataset.\")\n",
                "            num_batches_per_epoch = (num_total_samples + batch_size - 1) // batch_size\n",
                "            print(f\"Inferred total samples: {num_total_samples}, Inferred batches per epoch: {num_batches_per_epoch}\")\n",
                "        except Exception as e:\n",
                "            raise ValueError(\n",
                "                \"Dataset cardinality is unknown/infinite and could not be inferred. \"\n",
                "                \"Ensure your training dataset has a known size or provide steps_per_epoch. Error: {}\".format(e)\n",
                "            )\n",
                "    else:\n",
                "        num_batches_per_epoch = num_batches_per_epoch_tensor.numpy()\n",
                "    \n",
                "    if num_batches_per_epoch == 0:\n",
                "        raise ValueError(\"Number of batches per epoch is zero. Check dataset and batch size.\")\n",
                "    print(f\"Actual batches per epoch: {num_batches_per_epoch}\")\n",
                "    return num_batches_per_epoch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Function: Train Phase"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def train_phase(model, base_model, train_dataset, val_dataset, batch_size, \n",
                "                phase_name, epochs, initial_lr, lr_decay_alpha, callbacks_list, unfreeze_layers_count=None):\n",
                "    \"\"\"Helper function to run a training phase.\"\"\"\n",
                "    print(f\"\\n{phase_name}: Training for {epochs} epochs...\")\n",
                "\n",
                "    if unfreeze_layers_count is not None and base_model:\n",
                "        print(f\"Unfreezing last {unfreeze_layers_count} layers of the base model.\")\n",
                "        for layer in base_model.layers: # Freeze all first\n",
                "            layer.trainable = False\n",
                "        for layer in base_model.layers[-unfreeze_layers_count:]:\n",
                "            layer.trainable = True\n",
                "    elif base_model: # If unfreeze_layers_count is None, ensure base_model is frozen (for phase 1)\n",
                "         base_model.trainable = False\n",
                "\n",
                "\n",
                "    num_batches_per_epoch = get_num_batches_per_epoch(train_dataset, batch_size)\n",
                "    decay_steps = epochs * num_batches_per_epoch\n",
                "    print(f\"{phase_name} decay steps: {decay_steps}\")\n",
                "\n",
                "    lr_schedule = CosineDecay(\n",
                "        initial_learning_rate=initial_lr,\n",
                "        decay_steps=decay_steps,\n",
                "        alpha=lr_decay_alpha\n",
                "    )\n",
                "\n",
                "    model.compile(\n",
                "        optimizer=Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
                "        loss=CategoricalCrossentropy(label_smoothing=0.05),\n",
                "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), F1Score(name='f1')]\n",
                "    )\n",
                "\n",
                "    history = model.fit(\n",
                "        train_dataset,\n",
                "        validation_data=val_dataset,\n",
                "        epochs=epochs,\n",
                "        callbacks=callbacks_list,\n",
                "        verbose=1\n",
                "    )\n",
                "    return history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Model\n",
                "\n",
                "Function that implements the two-phase training approach."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def train_model(model, base_model, train_dataset, val_dataset, batch_size, epochs, \n",
                "               initial_lr_phase1, initial_lr_phase2, lr_decay_alpha,\n",
                "               checkpoint_filename='best_model.weights.h5', model_filename='emotion_recognition_model.h5'):\n",
                "    \n",
                "    checkpoint_path = os.path.join(Config.MODEL_DIR, checkpoint_filename)\n",
                "    model_save_path = os.path.join(Config.MODEL_DIR, model_filename)\n",
                "\n",
                "    if not os.path.exists(Config.MODEL_DIR):\n",
                "        os.makedirs(Config.MODEL_DIR)\n",
                "        \n",
                "    phase1_epochs = int(epochs * 0.4)\n",
                "    phase2_epochs = epochs - phase1_epochs\n",
                "    \n",
                "    # Callbacks for Phase 1\n",
                "    callbacks_phase1 = [\n",
                "        EarlyStopping(\n",
                "            monitor='val_f1', patience=10, mode='max', # Hardcoded patience\n",
                "            restore_best_weights=True, verbose=1, min_delta=0.001\n",
                "        )\n",
                "    ]\n",
                "    \n",
                "    history1 = train_phase(\n",
                "        model=model,\n",
                "        base_model=base_model,\n",
                "        train_dataset=train_dataset,\n",
                "        val_dataset=val_dataset,\n",
                "        batch_size=batch_size,\n",
                "        phase_name=\"Phase 1 (Frozen Base Model)\",\n",
                "        epochs=phase1_epochs,\n",
                "        initial_lr=initial_lr_phase1,\n",
                "        lr_decay_alpha=lr_decay_alpha,\n",
                "        callbacks_list=callbacks_phase1\n",
                "    )\n",
                "    \n",
                "    # Callbacks for Phase 2\n",
                "    callbacks_phase2 = [\n",
                "        ModelCheckpoint(\n",
                "            checkpoint_path, monitor='val_f1', save_best_only=True,\n",
                "            save_weights_only=True, mode='max', verbose=1\n",
                "        ),\n",
                "        EarlyStopping(\n",
                "            monitor='val_f1', patience=10, mode='max', # Hardcoded patience\n",
                "            restore_best_weights=True, verbose=1, min_delta=0.001\n",
                "        )\n",
                "    ]\n",
                "\n",
                "    history2 = train_phase(\n",
                "        model=model,\n",
                "        base_model=base_model,\n",
                "        train_dataset=train_dataset,\n",
                "        val_dataset=val_dataset,\n",
                "        batch_size=batch_size,\n",
                "        phase_name=\"Phase 2 (Fine-tuning)\",\n",
                "        epochs=phase2_epochs,\n",
                "        initial_lr=initial_lr_phase2,\n",
                "        lr_decay_alpha=lr_decay_alpha/10,\n",
                "        callbacks_list=callbacks_phase2,\n",
                "        unfreeze_layers_count=10 # Mild fine-tuning: unfreeze only last 10 layers\n",
                "    )\n",
                "    \n",
                "    model.save(model_save_path)\n",
                "    print(f\"Full model saved to {model_save_path}\")\n",
                "    \n",
                "    # Combine histories from both phases\n",
                "    combined_history = {}\n",
                "    for key in history1.history.keys():\n",
                "        combined_history[key] = history1.history[key] + history2.history.get(key, [])\n",
                "    for key in history2.history.keys(): # Add any keys unique to history2\n",
                "        if key not in combined_history:\n",
                "             combined_history[key] = history1.history.get(key, []) + history2.history[key]\n",
                "\n",
                "    return combined_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Model\n",
                "\n",
                "Function to evaluate the trained model on test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def evaluate_model(test_dataset, model_filename='emotion_recognition_model.h5'):\n",
                "    model_path = os.path.join(Config.MODEL_DIR, model_filename)\n",
                "    # Ensure the custom F1Score is available for loading\n",
                "    custom_objects = {'F1Score': F1Score}\n",
                "    try:\n",
                "        model = load_model(model_path, custom_objects=custom_objects)\n",
                "        print(\"Model loaded successfully with custom objects.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading model with custom objects: {e}. Trying without.\")\n",
                "        model = load_model(model_path) # Fallback if F1Score wasn't compiled in saved model\n",
                "        print(\"Model loaded successfully (fallback).\")\n",
                "\n",
                "\n",
                "    y_true_list = []\n",
                "    for _, y_batch in test_dataset:\n",
                "        y_true_list.append(y_batch.numpy())\n",
                "    \n",
                "    if not y_true_list:\n",
                "        print(\"Test dataset is empty or could not be iterated.\")\n",
                "        return {}\n",
                "        \n",
                "    y_true = np.concatenate(y_true_list, axis=0)\n",
                "\n",
                "    class_names = {0: \"Bored\", 1: \"Interested\", 2: \"Lacking_Focus\"}\n",
                "\n",
                "    y_pred_probs = model.predict(test_dataset, verbose=0)\n",
                "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
                "\n",
                "    if y_true.ndim > 1:\n",
                "        y_true = np.argmax(y_true, axis=1)\n",
                "\n",
                "    report = classification_report(y_true, y_pred, target_names=list(class_names.values()), output_dict=True, zero_division=0)\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_true, y_pred, target_names=list(class_names.values()), zero_division=0))\n",
                "\n",
                "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
                "    print(\"\\nConfusion Matrix:\")\n",
                "    print(conf_matrix)\n",
                "\n",
                "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
                "    for idx, emotion in class_names.items():\n",
                "        if idx < len(recall_per_class):\n",
                "             print(f\"Recall ({class_names[idx]}): {recall_per_class[idx]:.4f}\")\n",
                "\n",
                "    accuracy = accuracy_score(y_true, y_pred)\n",
                "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
                "\n",
                "    metrics_df = pd.DataFrame(report).transpose()\n",
                "    metrics_path = os.path.join(Config.MODEL_DIR, 'evaluation_metrics.csv')\n",
                "    metrics_df.to_csv(metrics_path)\n",
                "    print(f\"Detailed metrics saved to {metrics_path}\")\n",
                "\n",
                "    return {\n",
                "        'y_true': y_true, 'y_pred': y_pred, 'class_names': class_names,\n",
                "        'report': report, 'confusion_matrix': conf_matrix, 'accuracy': accuracy\n",
                "    }"
            ]
        }
    ]
}